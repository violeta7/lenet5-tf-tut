{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 미래에 python3로의 호환성 확보를 위함\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# 압축 풀고, 파일을 다루고, 시간을 재고, ...\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# 그림/그래프 출력\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = 'data'\n",
    "IMAGE_SIZE = 28  # MNIST는 28x28 흑백 이미지들로 구성됩니다.\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 31725\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 1000\n",
    "EVAL_FREQUENCY = 200  # Number of steps between evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the data.\n",
    "train_data_filename = 'train-images-idx3-ubyte.gz'\n",
    "train_labels_filename ='train-labels-idx1-ubyte.gz'\n",
    "test_data_filename = 't10k-images-idx3-ubyte.gz'\n",
    "test_labels_filename = 't10k-labels-idx1-ubyte.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "\n",
    "  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "  \"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(WORK_DIRECTORY+'/'+filename) as bytestream:\n",
    "    bytestream.read(16)\n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
    "    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n",
    "    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_labels(filename, num_images):\n",
    "  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(WORK_DIRECTORY+'/'+filename) as bytestream:\n",
    "    bytestream.read(8)\n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train-images-idx3-ubyte.gz\n",
      "Extracting train-labels-idx1-ubyte.gz\n",
      "Extracting t10k-images-idx3-ubyte.gz\n",
      "Extracting t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Extract it into numpy arrays.\n",
    "train_data = extract_data(train_data_filename, 60000)\n",
    "train_labels = extract_labels(train_labels_filename, 60000)\n",
    "test_data = extract_data(test_data_filename, 10000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000,)\n",
      "(10000, 28, 28, 1) (10000,)\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbe6f1aaef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+5Ke6Lp0ZxEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe72542860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_data.shape,train_labels.shape)\n",
    "print(test_data.shape,test_labels.shape)\n",
    "print(train_labels[1])\n",
    "plt.imshow(train_data[1].reshape(28,28), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a validation set.\n",
    "validation_data = train_data[:VALIDATION_SIZE, ...]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, ...]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The variables below hold all the trainable weights. They are passed an\n",
    "# initial value which will be assigned when we call:\n",
    "# {tf.global_variables_initializer().run()}\n",
    "\n",
    "# 첫번째 conv layer를 위한 weight와 bias, 5x5 filter, depth 6\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, NUM_CHANNELS, 6],  \n",
    "                      stddev=0.1,\n",
    "                      seed=SEED, dtype=tf.float32))\n",
    "conv1_biases = tf.Variable(tf.zeros([6], dtype=tf.float32))\n",
    "\n",
    "# 두번째 conv layer를 위한 weight와 bias, 5x5 filter, depth 16\n",
    "conv2_weights = tf.Variable(tf.truncated_normal(  \n",
    "  [5, 5, 6, 16], stddev=0.1,\n",
    "  seed=SEED, dtype=tf.float32))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[16], dtype=tf.float32))\n",
    "\n",
    "# 세번째 conv layer를 위한 weight와 bias, 5x5 filter, depth 120\n",
    "conv3_weights = tf.Variable(tf.truncated_normal(  \n",
    "  [5, 5, 16, 120], stddev=0.1,\n",
    "  seed=SEED, dtype=tf.float32))\n",
    "conv3_biases = tf.Variable(tf.constant(0.1, shape=[120], dtype=tf.float32))\n",
    "\n",
    "# 첫번째 fully connected layer, depth 84\n",
    "fc1_weights = tf.Variable(  \n",
    "  tf.truncated_normal([120, 84],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED,\n",
    "                      dtype=tf.float32))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[84], dtype=tf.float32))\n",
    "\n",
    "# LeNet-5의 마지막 layer이자, 두번째 fully connected layer, depth 10\n",
    "fc2_weights = tf.Variable(tf.truncated_normal([84, NUM_LABELS],  \n",
    "                                            stddev=0.1,\n",
    "                                            seed=SEED,\n",
    "                                            dtype=tf.float32))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 28, 28, 6]\n",
      "[None, 14, 14, 6]\n",
      "[None, 10, 10, 16]\n",
      "[None, 5, 5, 16]\n",
      "[None, 1, 1, 120]\n",
      "[None, 120]\n",
      "[None, 84]\n",
      "[None, 10]\n"
     ]
    }
   ],
   "source": [
    "# We will replicate the model structure for the training subgraph, as well\n",
    "# as the evaluation subgraphs, while sharing the trainable parameters.\n",
    "\"\"\"The Model definition.\"\"\"\n",
    "\n",
    "# placeholder를 선언한다. {x: 데이터, y: 레이블}\n",
    "x = tf.placeholder(tf.float32, [None, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS])\n",
    "y = tf.placeholder(tf.int64, [None,])\n",
    "\n",
    "# 1. 첫번재 conv layer\n",
    "# 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "# the same size as the input). Note that {strides} is a 4D array whose\n",
    "# shape matches the data layout: [image index, y, x, depth].\n",
    "conv = tf.nn.conv2d(x,\n",
    "                conv1_weights,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='SAME')\n",
    "\n",
    "actv = tf.nn.tanh(tf.nn.bias_add(conv, conv1_biases))\n",
    "print(actv.get_shape().as_list())\n",
    "\n",
    "# 2. 첫번째 Max pooling layer, LeNet-5에서는 Subsampling layer라고 불렀다.\n",
    "# Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "# the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "pool = tf.nn.max_pool(actv,\n",
    "                  ksize=[1, 2, 2, 1],\n",
    "                  strides=[1, 2, 2, 1],\n",
    "                  padding='SAME')\n",
    "print(pool.get_shape().as_list())\n",
    "\n",
    "# 3. 두번째 conv layer\n",
    "conv = tf.nn.conv2d(pool,\n",
    "                conv2_weights,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID')\n",
    "\n",
    "actv = tf.nn.tanh(tf.nn.bias_add(conv, conv2_biases))\n",
    "print(actv.get_shape().as_list())\n",
    "\n",
    "# 4. 두번째 max pooling layer\n",
    "pool = tf.nn.max_pool(actv,\n",
    "                  ksize=[1, 2, 2, 1],\n",
    "                  strides=[1, 2, 2, 1],\n",
    "                  padding='SAME')\n",
    "print(pool.get_shape().as_list())\n",
    "\n",
    "# 5. 세번째 conv layer\n",
    "conv = tf.nn.conv2d(pool,\n",
    "                conv3_weights,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID')\n",
    "\n",
    "actv = tf.nn.tanh(tf.nn.bias_add(conv, conv3_biases))\n",
    "actv_shape = actv.get_shape().as_list()\n",
    "print(actv_shape)\n",
    "\n",
    "# fully connected layer 연산을 적용하기 전에, vector로 변환이 필요하다. (여기서는 batch가 있기 때문에 2차원)\n",
    "reshape = tf.reshape(\n",
    "    actv,\n",
    "    [-1, actv_shape[1] * actv_shape[2] * actv_shape[3]])\n",
    "print(reshape.get_shape().as_list())\n",
    "\n",
    "# 6. 첫번째 fully-connected layer\n",
    "hidden = tf.nn.tanh(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "print(hidden.get_shape().as_list())\n",
    "# adding dropout?\n",
    "\n",
    "# 7. 두번째 fully-connected layer이자, 네트워크의 최종 결과 (one-hot encode 되어 있다.)\n",
    "logits = tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "print(logits.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "# tf.reduce mean: batch 단위로 나온 loss를 하나로 합쳐 평균을 구해주는 함수\n",
    "# tf.nn.sparse_softmax_cross_entropy_with_logits: one-hot encode되어 있는 최종 output을 가지고,\n",
    "#                                                 softmax + cross entropy loss를 구한다.\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=y, logits=logits))\n",
    "# L2 regularizers can be added!\n",
    "# L2 regularization for the fully connected parameters.\n",
    "#regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc2_weights))\n",
    "# Add the regularization term to the loss.\n",
    "#loss += 5e-4 * regularizers\n",
    "# Predictions for the current training minibatch.\n",
    "\n",
    "# prediction의 경우에는 간단하게, 10개의 vector에서 softmax를 통해 결과값(확률)을 예측하면 그만이다.\n",
    "prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_rate(predictions, labels):\n",
    "  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n",
    "  return 100.0 - (\n",
    "      100.0 *\n",
    "      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n",
    "      predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000 5000 5\n"
     ]
    }
   ],
   "source": [
    "num_epochs = NUM_EPOCHS\n",
    "train_size = train_labels.shape[0]\n",
    "validation_size = validation_labels.shape[0]\n",
    "val_steps = validation_size//EVAL_BATCH_SIZE\n",
    "print(train_size, validation_size, val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "Step 0 (epoch 0.00), 2098.2 ms\n",
      "Minibatch loss: 2.227, learning rate: 0.100000\n",
      "Minibatch error: 84.0%\n",
      "Validation error: 90.3%\n",
      "Step 200 (epoch 0.18), 445.0 ms\n",
      "Minibatch loss: 0.093, learning rate: 0.100000\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 7.7%\n",
      "Step 400 (epoch 0.36), 351.8 ms\n",
      "Minibatch loss: 0.055, learning rate: 0.100000\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 5.5%\n",
      "Step 600 (epoch 0.55), 331.1 ms\n",
      "Minibatch loss: 0.028, learning rate: 0.100000\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 4.2%\n",
      "Step 800 (epoch 0.73), 598.1 ms\n",
      "Minibatch loss: 0.060, learning rate: 0.100000\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 4.4%\n",
      "Step 1000 (epoch 0.91), 439.9 ms\n",
      "Minibatch loss: 0.033, learning rate: 0.100000\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 3.7%\n",
      "Step 1200 (epoch 1.09), 467.2 ms\n",
      "Minibatch loss: 0.039, learning rate: 0.085000\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 3.5%\n",
      "Step 1400 (epoch 1.27), 605.4 ms\n",
      "Minibatch loss: 0.038, learning rate: 0.085000\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 3.3%\n",
      "Step 1600 (epoch 1.45), 436.8 ms\n",
      "Minibatch loss: 0.053, learning rate: 0.085000\n",
      "Minibatch error: 2.0%\n",
      "Validation error: 2.6%\n",
      "Step 1800 (epoch 1.64), 441.8 ms\n",
      "Minibatch loss: 0.008, learning rate: 0.085000\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.1%\n",
      "Step 2000 (epoch 1.82), 633.5 ms\n",
      "Minibatch loss: 0.015, learning rate: 0.085000\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.7%\n",
      "Step 2200 (epoch 2.00), 453.6 ms\n",
      "Minibatch loss: 0.031, learning rate: 0.072250\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.6%\n",
      "Step 2400 (epoch 2.18), 468.8 ms\n",
      "Minibatch loss: 0.022, learning rate: 0.072250\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.4%\n",
      "Step 2600 (epoch 2.36), 505.8 ms\n",
      "Minibatch loss: 0.008, learning rate: 0.072250\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.4%\n",
      "Step 2800 (epoch 2.55), 494.8 ms\n",
      "Minibatch loss: 0.014, learning rate: 0.072250\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.2%\n",
      "Step 3000 (epoch 2.73), 508.0 ms\n",
      "Minibatch loss: 0.059, learning rate: 0.072250\n",
      "Minibatch error: 2.0%\n",
      "Validation error: 2.1%\n",
      "Step 3200 (epoch 2.91), 592.2 ms\n",
      "Minibatch loss: 0.011, learning rate: 0.072250\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.2%\n",
      "Step 3400 (epoch 3.09), 551.2 ms\n",
      "Minibatch loss: 0.011, learning rate: 0.061413\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.2%\n",
      "Step 3600 (epoch 3.27), 510.7 ms\n",
      "Minibatch loss: 0.025, learning rate: 0.061413\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.0%\n",
      "Step 3800 (epoch 3.45), 430.4 ms\n",
      "Minibatch loss: 0.013, learning rate: 0.061413\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 4000 (epoch 3.64), 426.3 ms\n",
      "Minibatch loss: 0.003, learning rate: 0.061413\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.8%\n",
      "Step 4200 (epoch 3.82), 449.1 ms\n",
      "Minibatch loss: 0.010, learning rate: 0.061413\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.8%\n",
      "Step 4400 (epoch 4.00), 645.3 ms\n",
      "Minibatch loss: 0.008, learning rate: 0.052201\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.3%\n",
      "Step 4600 (epoch 4.18), 448.7 ms\n",
      "Minibatch loss: 0.006, learning rate: 0.052201\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.1%\n",
      "Step 4800 (epoch 4.36), 428.7 ms\n",
      "Minibatch loss: 0.009, learning rate: 0.052201\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.1%\n",
      "Step 5000 (epoch 4.55), 440.8 ms\n",
      "Minibatch loss: 0.062, learning rate: 0.052201\n",
      "Minibatch error: 2.0%\n",
      "Validation error: 1.8%\n",
      "Step 5200 (epoch 4.73), 487.4 ms\n",
      "Minibatch loss: 0.003, learning rate: 0.052201\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.1%\n",
      "Step 5400 (epoch 4.91), 551.9 ms\n",
      "Minibatch loss: 0.013, learning rate: 0.052201\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.8%\n",
      "Step 5600 (epoch 5.09), 428.7 ms\n",
      "Minibatch loss: 0.022, learning rate: 0.044371\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.0%\n",
      "Step 5800 (epoch 5.27), 445.5 ms\n",
      "Minibatch loss: 0.006, learning rate: 0.044371\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.0%\n",
      "Step 6000 (epoch 5.45), 418.2 ms\n",
      "Minibatch loss: 0.002, learning rate: 0.044371\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 6200 (epoch 5.64), 434.1 ms\n",
      "Minibatch loss: 0.016, learning rate: 0.044371\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 6400 (epoch 5.82), 584.6 ms\n",
      "Minibatch loss: 0.001, learning rate: 0.044371\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.6%\n",
      "Step 6600 (epoch 6.00), 440.0 ms\n",
      "Minibatch loss: 0.011, learning rate: 0.037715\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.1%\n",
      "Step 6800 (epoch 6.18), 450.1 ms\n",
      "Minibatch loss: 0.013, learning rate: 0.037715\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.8%\n",
      "Step 7000 (epoch 6.36), 446.2 ms\n",
      "Minibatch loss: 0.015, learning rate: 0.037715\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.0%\n",
      "Step 7200 (epoch 6.55), 469.9 ms\n",
      "Minibatch loss: 0.014, learning rate: 0.037715\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.5%\n",
      "Step 7400 (epoch 6.73), 546.1 ms\n",
      "Minibatch loss: 0.003, learning rate: 0.037715\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 7600 (epoch 6.91), 429.2 ms\n",
      "Minibatch loss: 0.037, learning rate: 0.037715\n",
      "Minibatch error: 2.0%\n",
      "Validation error: 1.7%\n",
      "Step 7800 (epoch 7.09), 449.7 ms\n",
      "Minibatch loss: 0.014, learning rate: 0.032058\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.9%\n",
      "Step 8000 (epoch 7.27), 613.0 ms\n",
      "Minibatch loss: 0.018, learning rate: 0.032058\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.8%\n",
      "Step 8200 (epoch 7.45), 508.3 ms\n",
      "Minibatch loss: 0.008, learning rate: 0.032058\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 8400 (epoch 7.64), 439.5 ms\n",
      "Minibatch loss: 0.020, learning rate: 0.032058\n",
      "Minibatch error: 2.0%\n",
      "Validation error: 1.8%\n",
      "Step 8600 (epoch 7.82), 454.0 ms\n",
      "Minibatch loss: 0.012, learning rate: 0.032058\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.5%\n",
      "Step 8800 (epoch 8.00), 469.3 ms\n",
      "Minibatch loss: 0.005, learning rate: 0.027249\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.0%\n",
      "Step 9000 (epoch 8.18), 423.3 ms\n",
      "Minibatch loss: 0.015, learning rate: 0.027249\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.8%\n",
      "Step 9200 (epoch 8.36), 449.3 ms\n",
      "Minibatch loss: 0.002, learning rate: 0.027249\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.0%\n",
      "Step 9400 (epoch 8.55), 486.6 ms\n",
      "Minibatch loss: 0.009, learning rate: 0.027249\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.5%\n",
      "Step 9600 (epoch 8.73), 614.2 ms\n",
      "Minibatch loss: 0.010, learning rate: 0.027249\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 9800 (epoch 8.91), 498.2 ms\n",
      "Minibatch loss: 0.008, learning rate: 0.027249\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.8%\n",
      "Step 10000 (epoch 9.09), 435.2 ms\n",
      "Minibatch loss: 0.001, learning rate: 0.023162\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.6%\n",
      "Step 10200 (epoch 9.27), 433.7 ms\n",
      "Minibatch loss: 0.001, learning rate: 0.023162\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.8%\n",
      "Step 10400 (epoch 9.45), 669.8 ms\n",
      "Minibatch loss: 0.006, learning rate: 0.023162\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 10600 (epoch 9.64), 444.1 ms\n",
      "Minibatch loss: 0.006, learning rate: 0.023162\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 10800 (epoch 9.82), 449.1 ms\n",
      "Minibatch loss: 0.004, learning rate: 0.023162\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.5%\n",
      "Step 11000 (epoch 10.00), 610.3 ms\n",
      "Minibatch loss: 0.014, learning rate: 0.019687\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.0%\n",
      "Test error: 1.4%\n"
     ]
    }
   ],
   "source": [
    "# global_step은 mini-batch마다 1씩 증가하면서 learning_rate decay를 수행해주는 변수\n",
    "global_step = tf.Variable(0, dtype=tf.float32)\n",
    "\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.1.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  learning_rate = 0.1,                # Base learning rate.\n",
    "  global_step = global_step * BATCH_SIZE,  # Current index into the dataset.\n",
    "  decay_steps = train_size,          # Decay step.\n",
    "  decay_rate = 0.85,                # Decay rate.\n",
    "  staircase=True)\n",
    "\n",
    "# Use simple sgd for the optimization.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "# try other optimizer?\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate,momentum=0.9).minimize(loss,global_step=global_step)\n",
    "\n",
    "# Create a local session to run the training.\n",
    "start_time = time.time()\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options={'allow_growth':True})) as sess:\n",
    "  \n",
    "  # Run all the initializers to prepare the trainable parameters.\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized!')\n",
    "    \n",
    "  # Loop through training steps.\n",
    "  for step in range(int(num_epochs * train_size) // BATCH_SIZE + 1):\n",
    "    # Compute the offset of the current minibatch in the data.\n",
    "    # Note that we could use better randomization across epochs.\n",
    "    # 여기서는 전체 데이터 중 mini-batch만큼을 떼어내서 학습을 진행하는데 그 mini-batch별 인덱스를 계산하고 있다. (offset)\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    \n",
    "    # This dictionary maps the batch data (as a numpy array) to the\n",
    "    # node in the graph it should be fed to.\n",
    "    feed_dict = {x: batch_data,\n",
    "               y: batch_labels}\n",
    "    \n",
    "    # Run the optimizer to update weights.\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "    \n",
    "    # EVAL_FREQUENCY 횟수만큼 mini-batch를 수행했을 때, 밸리데이션셋에 적용하여 훈련 성과를 본다.\n",
    "    # print some extra information once reach the evaluation frequency\n",
    "    if step % EVAL_FREQUENCY == 0:\n",
    "      # fetch some extra nodes' data\n",
    "      l, lr, predictions = sess.run([loss, learning_rate, prediction],feed_dict=feed_dict)\n",
    "        \n",
    "      val_offset = (step//EVAL_FREQUENCY)%val_steps\n",
    "      #print(val_offset)\n",
    "      val_batch_data = validation_data[val_offset:val_offset+EVAL_BATCH_SIZE]\n",
    "      feed_dict = {x: val_batch_data}\n",
    "      val_predictions = sess.run([prediction],feed_dict=feed_dict)[0]\n",
    "        \n",
    "      elapsed_time = time.time() - start_time\n",
    "      print('Step %d (epoch %.2f), %.1f ms' % (step, float(step) * BATCH_SIZE / train_size, 1000 * elapsed_time))\n",
    "      print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "      print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n",
    "      print('Validation error: %.1f%%' % error_rate(val_predictions, validation_labels[val_offset:val_offset+EVAL_BATCH_SIZE]))\n",
    "      sys.stdout.flush()\n",
    "      start_time = time.time()\n",
    "\n",
    "  # Finally print the result!\n",
    "  # 최종적으로 훈련이 끝나면, 훈련한 모델을 테스트셋에 적용하여 결과를 본다.\n",
    "  feed_dict = {x: test_data}\n",
    "  test_predictions = sess.run([prediction],feed_dict=feed_dict)[0]\n",
    "  test_error = error_rate(test_predictions, test_labels)\n",
    "  print('Test error: %.1f%%' % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
