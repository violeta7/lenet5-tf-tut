{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7f6ccd361997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 그림/그래프 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</home/keeyoung/anaconda3/envs/tensorflow/lib/python3.7/site-packages/decorator.py:decorator-gen-108>\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3400\u001b[0m         \"\"\"\n\u001b[1;32m   3401\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylabtools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3402\u001b[0;31m         \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mfind_gui_and_backend\u001b[0;34m(gui, gui_select)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \"\"\"\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# 미래에 python3로의 호환성 확보를 위함\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# 압축 풀고, 파일을 다루고, 시간을 재고, ...\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# 그림/그래프 출력\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = 'data'\n",
    "IMAGE_SIZE = 28  # MNIST는 28x28 흑백 이미지들로 구성됩니다.\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 31725\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 1000\n",
    "EVAL_FREQUENCY = 200  # Number of steps between evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data.\n",
    "train_data_filename = 'train-images-idx3-ubyte.gz'\n",
    "train_labels_filename ='train-labels-idx1-ubyte.gz'\n",
    "test_data_filename = 't10k-images-idx3-ubyte.gz'\n",
    "test_labels_filename = 't10k-labels-idx1-ubyte.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "\n",
    "  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "  \"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(WORK_DIRECTORY+'/'+filename) as bytestream:\n",
    "    bytestream.read(16)\n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
    "    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n",
    "    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(filename, num_images):\n",
    "  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(WORK_DIRECTORY+'/'+filename) as bytestream:\n",
    "    bytestream.read(8)\n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract it into numpy arrays.\n",
    "train_data = extract_data(train_data_filename, 60000)\n",
    "train_labels = extract_labels(train_labels_filename, 60000)\n",
    "test_data = extract_data(test_data_filename, 10000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape,train_labels.shape)\n",
    "print(test_data.shape,test_labels.shape)\n",
    "print(train_labels[1])\n",
    "plt.imshow(train_data[1].reshape(28,28), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a validation set.\n",
    "validation_data = train_data[:VALIDATION_SIZE, ...]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, ...]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variables below hold all the trainable weights. They are passed an\n",
    "# initial value which will be assigned when we call:\n",
    "# {tf.global_variables_initializer().run()}\n",
    "\n",
    "# 첫번째 conv layer를 위한 weight와 bias, 5x5 filter, depth 6\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, NUM_CHANNELS, 6],  \n",
    "                      stddev=0.1,\n",
    "                      seed=SEED, dtype=tf.float32))\n",
    "conv1_biases = tf.Variable(tf.zeros([6], dtype=tf.float32))\n",
    "\n",
    "# 두번째 conv layer를 위한 weight와 bias, 5x5 filter, depth 16\n",
    "conv2_weights = tf.Variable(tf.truncated_normal(  \n",
    "  [5, 5, 6, 16], stddev=0.1,\n",
    "  seed=SEED, dtype=tf.float32))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[16], dtype=tf.float32))\n",
    "\n",
    "# 세번째 conv layer를 위한 weight와 bias, 5x5 filter, depth 120\n",
    "conv3_weights = tf.Variable(tf.truncated_normal(  \n",
    "  [5, 5, 16, 120], stddev=0.1,\n",
    "  seed=SEED, dtype=tf.float32))\n",
    "conv3_biases = tf.Variable(tf.constant(0.1, shape=[120], dtype=tf.float32))\n",
    "\n",
    "# 첫번째 fully connected layer, depth 84\n",
    "fc1_weights = tf.Variable(  \n",
    "  tf.truncated_normal([120, 84],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED,\n",
    "                      dtype=tf.float32))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[84], dtype=tf.float32))\n",
    "\n",
    "# LeNet-5의 마지막 layer이자, 두번째 fully connected layer, depth 10\n",
    "fc2_weights = tf.Variable(tf.truncated_normal([84, NUM_LABELS],  \n",
    "                                            stddev=0.1,\n",
    "                                            seed=SEED,\n",
    "                                            dtype=tf.float32))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will replicate the model structure for the training subgraph, as well\n",
    "# as the evaluation subgraphs, while sharing the trainable parameters.\n",
    "\"\"\"The Model definition.\"\"\"\n",
    "\n",
    "# placeholder를 선언한다. {x: 데이터, y: 레이블}\n",
    "x = tf.placeholder(tf.float32, [None, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS])\n",
    "y = tf.placeholder(tf.int64, [None,])\n",
    "\n",
    "# 1. 첫번재 conv layer\n",
    "# 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "# the same size as the input). Note that {strides} is a 4D array whose\n",
    "# shape matches the data layout: [image index, y, x, depth].\n",
    "conv = tf.nn.conv2d(x,\n",
    "                conv1_weights,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='SAME')\n",
    "\n",
    "actv = tf.nn.tanh(tf.nn.bias_add(conv, conv1_biases))\n",
    "print(actv.get_shape().as_list())\n",
    "\n",
    "# 2. 첫번째 Max pooling layer, LeNet-5에서는 Subsampling layer라고 불렀다.\n",
    "# Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "# the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "pool = tf.nn.max_pool(actv,\n",
    "                  ksize=[1, 2, 2, 1],\n",
    "                  strides=[1, 2, 2, 1],\n",
    "                  padding='SAME')\n",
    "print(pool.get_shape().as_list())\n",
    "\n",
    "# 3. 두번째 conv layer\n",
    "conv = tf.nn.conv2d(pool,\n",
    "                conv2_weights,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID')\n",
    "\n",
    "actv = tf.nn.tanh(tf.nn.bias_add(conv, conv2_biases))\n",
    "print(actv.get_shape().as_list())\n",
    "\n",
    "# 4. 두번째 max pooling layer\n",
    "pool = tf.nn.max_pool(actv,\n",
    "                  ksize=[1, 2, 2, 1],\n",
    "                  strides=[1, 2, 2, 1],\n",
    "                  padding='SAME')\n",
    "print(pool.get_shape().as_list())\n",
    "\n",
    "# 5. 세번째 conv layer\n",
    "conv = tf.nn.conv2d(pool,\n",
    "                conv3_weights,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID')\n",
    "\n",
    "actv = tf.nn.tanh(tf.nn.bias_add(conv, conv3_biases))\n",
    "actv_shape = actv.get_shape().as_list()\n",
    "print(actv_shape)\n",
    "\n",
    "# fully connected layer 연산을 적용하기 전에, vector로 변환이 필요하다. (여기서는 batch가 있기 때문에 2차원)\n",
    "reshape = tf.reshape(\n",
    "    actv,\n",
    "    [-1, actv_shape[1] * actv_shape[2] * actv_shape[3]])\n",
    "print(reshape.get_shape().as_list())\n",
    "\n",
    "# 6. 첫번째 fully-connected layer\n",
    "hidden = tf.nn.tanh(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "print(hidden.get_shape().as_list())\n",
    "# adding dropout?\n",
    "\n",
    "# 7. 두번째 fully-connected layer이자, 네트워크의 최종 결과 (one-hot encode 되어 있다.)\n",
    "logits = tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "print(logits.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "# tf.reduce mean: batch 단위로 나온 loss를 하나로 합쳐 평균을 구해주는 함수\n",
    "# tf.nn.sparse_softmax_cross_entropy_with_logits: one-hot encode되어 있는 최종 output을 가지고,\n",
    "#                                                 softmax + cross entropy loss를 구한다.\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=y, logits=logits))\n",
    "# L2 regularizers can be added!\n",
    "# L2 regularization for the fully connected parameters.\n",
    "#regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc2_weights))\n",
    "# Add the regularization term to the loss.\n",
    "#loss += 5e-4 * regularizers\n",
    "# Predictions for the current training minibatch.\n",
    "\n",
    "# prediction의 경우에는 간단하게, 10개의 vector에서 softmax를 통해 결과값(확률)을 예측하면 그만이다.\n",
    "prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(predictions, labels):\n",
    "  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n",
    "  return 100.0 - (\n",
    "      100.0 *\n",
    "      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n",
    "      predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = NUM_EPOCHS\n",
    "train_size = train_labels.shape[0]\n",
    "validation_size = validation_labels.shape[0]\n",
    "val_steps = validation_size//EVAL_BATCH_SIZE\n",
    "print(train_size, validation_size, val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_step은 mini-batch마다 1씩 증가하면서 learning_rate decay를 수행해주는 변수\n",
    "global_step = tf.Variable(0, dtype=tf.float32)\n",
    "\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.1.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  learning_rate = 0.1,                # Base learning rate.\n",
    "  global_step = global_step * BATCH_SIZE,  # Current index into the dataset.\n",
    "  decay_steps = train_size,          # Decay step.\n",
    "  decay_rate = 0.85,                # Decay rate.\n",
    "  staircase=True)\n",
    "\n",
    "# Use simple sgd for the optimization.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "# try other optimizer?\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate,momentum=0.9).minimize(loss,global_step=global_step)\n",
    "\n",
    "# Create a local session to run the training.\n",
    "start_time = time.time()\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options={'allow_growth':True})) as sess:\n",
    "  \n",
    "  # Run all the initializers to prepare the trainable parameters.\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized!')\n",
    "    \n",
    "  # Loop through training steps.\n",
    "  for step in range(int(num_epochs * train_size) // BATCH_SIZE + 1):\n",
    "    # Compute the offset of the current minibatch in the data.\n",
    "    # Note that we could use better randomization across epochs.\n",
    "    # 여기서는 전체 데이터 중 mini-batch만큼을 떼어내서 학습을 진행하는데 그 mini-batch별 인덱스를 계산하고 있다. (offset)\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    \n",
    "    # This dictionary maps the batch data (as a numpy array) to the\n",
    "    # node in the graph it should be fed to.\n",
    "    feed_dict = {x: batch_data,\n",
    "               y: batch_labels}\n",
    "    \n",
    "    # Run the optimizer to update weights.\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "    \n",
    "    # EVAL_FREQUENCY 횟수만큼 mini-batch를 수행했을 때, 밸리데이션셋에 적용하여 훈련 성과를 본다.\n",
    "    # print some extra information once reach the evaluation frequency\n",
    "    if step % EVAL_FREQUENCY == 0:\n",
    "      # fetch some extra nodes' data\n",
    "      l, lr, predictions = sess.run([loss, learning_rate, prediction],feed_dict=feed_dict)\n",
    "        \n",
    "      val_offset = (step//EVAL_FREQUENCY)%val_steps\n",
    "      #print(val_offset)\n",
    "      val_batch_data = validation_data[val_offset:val_offset+EVAL_BATCH_SIZE]\n",
    "      feed_dict = {x: val_batch_data}\n",
    "      val_predictions = sess.run([prediction],feed_dict=feed_dict)[0]\n",
    "        \n",
    "      elapsed_time = time.time() - start_time\n",
    "      print('Step %d (epoch %.2f), %.1f ms' % (step, float(step) * BATCH_SIZE / train_size, 1000 * elapsed_time))\n",
    "      print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "      print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n",
    "      print('Validation error: %.1f%%' % error_rate(val_predictions, validation_labels[val_offset:val_offset+EVAL_BATCH_SIZE]))\n",
    "      sys.stdout.flush()\n",
    "      start_time = time.time()\n",
    "\n",
    "  # Finally print the result!\n",
    "  # 최종적으로 훈련이 끝나면, 훈련한 모델을 테스트셋에 적용하여 결과를 본다.\n",
    "  feed_dict = {x: test_data}\n",
    "  test_predictions = sess.run([prediction],feed_dict=feed_dict)[0]\n",
    "  test_error = error_rate(test_predictions, test_labels)\n",
    "  print('Test error: %.1f%%' % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
